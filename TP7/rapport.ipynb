{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rapport du TP7 :  Machine à Vecteur de Support (MVS)\n",
    "Le but de ce rapport est d’expliquer le fonctionnement des Machine à Vecteur de Support et l’utilité de ses paramètres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MVS est chargée de trouver la limite de décision pour séparer les différentes classes et maximiser la marge.\n",
    "\n",
    "Les marges sont les distances (perpendiculaires) entre l'hyperplan séparateur et les points les plus proches de lui.    \n",
    "\n",
    "\n",
    "## MVS dans les datasets linéairement séparables\n",
    "<img src=\"linear.png\" align=\"center\"/>\n",
    "\n",
    "Évidemment, un nombre infini d'hyperplan existent pour séparer les points marron et bleu dans l'exemple ci-dessus. \n",
    "\n",
    "MVS doit trouver l'hyperplan optimale telle que :\n",
    "\n",
    "- Ne prendre en considération que les hyperplans séparateur i.e les hyperplan qui classent les individus correctement.\n",
    "- choisir l'hyperplan qui maximise la marge.\n",
    "\n",
    "### Qu'est-ce qu'un hyperplan?\n",
    "\n",
    "\n",
    "Un hyperplan est un sous-espace à $n-1$ dimensions pour un espace à $n$ dimensions. Pour un espace à 2 dimensions, son hyperplan sera à 1 dimension, qui n'est qu'une ligne. Pour un espace à 3 dimensions, son hyperplan sera à 2 dimensions, qui est un plan qui tranche le cube...etc.\n",
    "\n",
    "Tout hyperplan de dimension $n$ peut être écrit mathématiquement comme ci-dessous:\n",
    "\n",
    "$$ \\theta _0 + \\theta _1 * x_1 + \\theta _2 * x_2 + ... + \\theta _n * x_n  = 0 $$\n",
    "\n",
    "### Qu'est-ce qu'un hyperplan séparateur?\n",
    "<img src=\"linear2.png\" align=\"center\"/>\n",
    "\n",
    "Dans l'exemple ci dessus la ligne tracée (ou hyperplan unidimensionnel) est un hyperplan séparateur, car tout les éléments marron sont au dessous de la ligne, tandis que tout les éléments bleus sont au dessus.\n",
    "\n",
    "Cette propriété peut s'écrire mathématiquement comme suit:\n",
    "\n",
    "$$ \\theta _0 + \\theta _1 * x_1 + \\theta _2 * x_2 > 0 \\quad \\text{si y=1 (bleu)}$$\n",
    "$$ \\theta _0 + \\theta _1 * x_1 + \\theta _2 * x_2 < 0 \\quad \\text{si y=-1 (marron)}$$\n",
    "\n",
    "En généralisant ces deux inégalités en une seule, on obtient :\n",
    "\n",
    "$$y*(\\theta _0 + \\theta _1 * x_1 + \\theta _2 * x_2) > 0$$\n",
    "\n",
    "Et cette dernière représente la contrainte d'hyperplan séparateur.\n",
    "\n",
    "## MVS dans les datasets non-linéairement séparables\n",
    "\n",
    "Dans le cas linéairement séparable, MVS essaie de trouver l'hyperplan qui maximise la marge, à condition que tout les individus soient correctement classées. Mais en réalité, les datasets ne sont presque jamais linéairement séparables.\n",
    "MVS traite les cas non linéairement séparables en introduisant deux concepts : les marges souples et les kernels.\n",
    "\n",
    "### Les marges souples\n",
    "avec Les marges souples, MVS tolère que quelques points soient mal classés et essaie d'équilibrer le compromis entre la recherche d'une ligne qui maximise la marge et minimise la mauvaise classification.\n",
    "\n",
    "Le degré de tolérance que nous voulons donner lors de la recherche de la limite de décision est un hyper-paramètre important pour le MVS. Dans Sklearn, il est représenté par le terme de pénalité - \"C\". Plus le C est grand, plus MVS est pénalisé lorsqu'il fait une mauvaise classification. Par conséquent, plus la marge est étroite et moins de vecteurs de support dépendront de la limite de décision.\n",
    "\n",
    "<img src=\"soft1.png\" align=\"center\"/>\n",
    "<img src=\"soft100.png\" align=\"center\"/>\n",
    "\n",
    "### Les kernels\n",
    "Ce qu'un kernel fait, c'est qu'il utilise des caractéristiques existantes, applique certaines transformations et crée de nouvelles caractéristiques. Ces nouvelles caractéristiques sont la clé pour MVS pour trouver la limite de décision non linéaire.\n",
    "\n",
    "Dans Sklearn - svm.SVC(), nous pouvons choisir 'linear', 'poly', 'rbf', 'sigmoide', 'precomputed' comme kernel. Dans notre TP on se concentre sur le kernel gaussien ou 'rbf', sa fonction est la suivante:\n",
    "\n",
    "$$K(x_1, x_2) = exp(-\\gamma \\|x_1 - x_2\\|^{2})$$\n",
    "\n",
    "Le terme $\\gamma$ définit l'influence qu'a chaque point sur le reste des autres points, plus $\\gamma$ est élevé, plus les points auront d'influence sur la limite de décision, plus la limite de decision sera irrégulière.\n",
    "\n",
    "<img src=\"rbf1.png\" align=\"left\"/>\n",
    "<img src=\"rbf25.png\" align=\"right\"/>\n",
    "<img src=\"rbf50.png\" align=\"center\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
